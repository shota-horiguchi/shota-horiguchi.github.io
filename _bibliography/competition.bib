@inproceedings{kamo2024ntt,
    title={{NTT} Multi-Speaker {ASR} System for the {DASR} Task of {CHiME-8} Challenge},
    author={Kamo, Naoyuki and Tawara, Naohiro and Ando, Atsushi and Kano, Takatomo and Sato, Hiroshi and Ikeshita, Rintaro and Moriya, Takafumi and Horiguchi, Shota and Matsuura, Kohei and Ogawa, Atsunori and Plaquet, Alexis and Ashihara, Takanori and Ochiai, Tsubasa and Mimura, Masato and Delcroix, Marc and Nakatani, Tomohiro and Asami, Taichi and Araki, Shoko},
    booktitle={The 8th International Workshop on Speech Processing in Everyday Environments (CHiME-2024)},
    equal_contribution={1,2},
    year={2024},
    month=sep,
    abbr={CHiME},
    abstract={We present a distant automatic speech recognition (DASR) system developed for the CHiME-8 DASR track. It consists of a diarization first pipeline. For diarization, we use end-to-end diarization with vector clustering (EEND-VC) followed by target speaker voice activity detection (TS-VAD) refinement. To deal with various numbers of speakers, we developed a new multi-channel speaker counting approach. We then apply guided source separation (GSS) with several improvements to the baseline system. Finally, we perform ASR using a combination of systems built from strong pre-trained models. Our proposed system achieves a macro tcpWER of 21.4 % on the dev set, which is a 57 % relative improvement over the baseline.},
    arxiv={2409.05554},
    html={https://www.isca-archive.org/chime_2024/kamo24_chime.html}
}
@inproceedings{horiguchi2022hitachijhu,
    title={The {Hitachi-JHU} {DIHARD} {III} System: Competitive End-to-End Neural Diarization and X-vector Clustering Systems Combined by {DOVER-Lap}},
    author={Horiguchi, Shota and Yalta, Nelson and Garcia, Paola and Takashima, Yuki and Xue, Yawen and Raj, Desh and Huang, Zili and Fujita, Yusuke and Watanabe, Shinji and Khudanpur, Sanjeev},
    booktitle={The Third DIHARD Speech Diarization Challenge (DIHARD III)},
    year={2021},
    month=jan,
    keywords={first},
    abbr={DIHARD},
    abstract={This paper provides a detailed description of the Hitachi-JHU system that was submitted to the Third DIHARD Speech Diarization Challenge. The system outputs the ensemble results of the five subsystems: two x-vector-based subsystems, two end-to-end neural diarization-based subsystems, and one hybrid subsystem. We refine each system and all five subsystems become competitive and complementary. After the DOVER-Lap based system combination, it achieved diarization error rates of 11.58 % and 14.09 % in Track 1 full and core, and 16.94 % and 20.01 % in Track 2 full and core, respectively. With their results, we won second place in all the tasks of the challenge.},
    arxiv={2102.01363},
    pdf={https://dihardchallenge.github.io/dihard3/system_descriptions/dihard3_system_description_team115.pdf},
    slides={https://dihardchallenge.github.io/dihard3workshop/slide/Hitachi-JHU%20System%20for%20the%20Third%20DIHARD%20Speech%20Diarization%20Challenge.pdf}
}
@inproceedings{watanabe2020chime6,
    title={{CHiME-6} {Challenge}: Tackling Multispeaker Speech Recognition for Unsegmented Recordings},
    author={Watanabe, Shinji and Mandel, Michael and Barker, Jon and Vincent, Emmanuel and Arora, Ashish and Chang, Xuankai and Khudanpur, Sanjeev and Manohar, Vimal and Povey, Daniel and Raj, Desh and Snyder, David and Subramanian, Aswin Shanmugam and Trmal, Jan and Yair, Bar Ben and Boeddeker, Christoph and Ni, Zhaoheng and Fujita, Yusuke and Horiguchi, Shota and Kanda, Naoyuki and Yoshioka, Takuya and Ryant, Neville},
    booktitle={The 6th International Workshop on Speech Processing in Everyday Environments (CHiME-2020)},
    year={2020},
    month=may,
    pages={1--7},
    abbr={CHiME},
    abstract={Following the success of the 1st, 2nd, 3rd, 4th and 5th CHiME challenges we organize the 6th CHiME Speech Separation and Recognition Challenge (CHiME-6). The new challenge revisits the previous CHiME-5 challenge and further considers the problem of distant multi-microphone conversational speech diarization and recognition in everyday home environments. Speech material is the same as the previous CHiME-5 recordings except for accurate array synchronization. The material was elicited using a dinner party scenario with efforts taken to capture data that is representative of natural conversational speech. This paper provides a baseline description of the CHiME-6 challenge for both segmented multispeaker speech recognition (Track 1) and unsegmented multispeaker speech recognition (Track 2). Of note, Track 2 is the first challenge activity in the community to tackle an unsegmented multispeaker speech recognition scenario with a complete set of reproducible open source baselines providing speech enhancement, speaker diarization, and speech recognition modules.},
    arxiv={2004.09249},
    html={https://www.isca-speech.org/archive/chime_2020/watanabe20b_chime.html}
}
@inproceedings{morishita2020hitachi,
    title={Hitachi at {SemEval-2020} Task 8: Simple but Effective Modality Ensemble for Meme Emotion Recognition},
    author={Morishita, Terufumi and Morio, Gaku and Horiguchi, Shota and Ozaki, Hiroaki and Miyoshi, Toshinori},
    booktitle={The Forteenth Workshop on Semantic Evaluation (SemEval)},
    year={2020},
    month=dec,
    pages={1126â€“-1134},
    abbr={SemEval},
    abstract={Users of social networking services often share their emotions via multi-modal content, usually images paired with text embedded in them. SemEval-2020 task 8, Memotion Analysis, aims at automatically recognizing these emotions of so-called internet memes. In this paper, we propose a simple but effective Modality Ensemble that incorporates visual and textual deep-learning models, which are independently trained, rather than providing a single multi-modal joint network. To this end, we first fine-tune four pre-trained visual models (i.e., Inception-ResNet, PolyNet, SENet, and PNASNet) and four textual models (i.e., BERT, GPT-2, Transformer-XL, and XLNet). Then, we fuse their predictions with ensemble methods to effectively capture cross-modal correlations. The experiments performed on dev-set show that both visual and textual features aided each other, especially in subtask-C, and consequently, our system ranked 2nd on subtask-C.},
    html={https://aclanthology.org/2020.semeval-1.149},
    equal_contribution={1,2}
}
@inproceedings{kanda2018hitachijhu,
    title={The {Hitachi/JHU} {CHiME-5} System: Advances in Speech Recognition for Everyday Home Environments Using Multiple Microphone Arrays},
    author={Naoyuki Kanda and Rintaro Ikeshita and Shota Horiguchi and Yusuke Fujita and Kenji Nagamatsu and Xiaofei Wang and Vimal Manohar and Nelson Enrique {Yalta Soplin} and Matthew Maciejewski and Szu-Jui Chen and Aswin Shanmugam Subramanian and Ruizhi Li and Zhiqi Wang and Jason Naradowsky and L. Paola Garcia-Perera and Gregory Sell},
    booktitle={The 5th International Workshop on Speech Processing in Everyday Environments (CHiME-2018)},
    year={2018},
    month=sep,
    pages={6--10},
    abbr={CHiME},
    abstract={This paper presents Hitachi and JHU's efforts on developing CHiME-5 system to recognize dinner party speeches recorded by multiple microphone arrays. We newly developed (1) the way to apply multiple data augmentation methods, (2) residual bidirectional long short-term memory, (3) 4-ch acoustic models, (4) multiple-array combination methods, (5) hypothesis deduplication method, and (6) speaker adaptation technique of neural beamformer. As the results, our best system in category B achieved 52.38% of word error rates (WERs) for development set, which corresponded to 35% of relative WER reduction from the state-of-the-art baseline. Our best system also achieved 48.20% of WER for evaluation set, which was the 2nd best result in the CHiME-5 competition.},
    html={https://www.isca-speech.org/archive/chime_2018/kanda18_chime.html}
}