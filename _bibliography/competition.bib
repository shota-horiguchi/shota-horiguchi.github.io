@inproceedings{horiguchi2022hitachijhu,
    title={The {Hitachi-JHU} {DIHARD} {III} System: Competitive End-to-End Neural Diarization and X-vector Clustering Systems Combined by {DOVER-Lap}},
    author={Horiguchi, Shota and Yalta, Nelson and Garcia, Paola and Takashima, Yuki and Xue, Yawen and Raj, Desh and Huang, Zili and Fujita, Yusuke and Watanabe, Shinji and Khudanpur, Sanjeev},
    booktitle={The Third DIHARD Speech Diarization Challenge (DIHARD III)},
    year={2022},
    keywords={first},
    abbr={DIHARD},
    abstract={This paper provides a detailed description of the Hitachi-JHU system that was submitted to the Third DIHARD Speech Diarization Challenge. The system outputs the ensemble results of the five subsystems: two x-vector-based subsystems, two end-to-end neural diarization-based subsystems, and one hybrid subsystem. We refine each system and all five subsystems become competitive and complementary. After the DOVER-Lap based system combination, it achieved diarization error rates of 11.58 % and 14.09 % in Track 1 full and core, and 16.94 % and 20.01 % in Track 2 full and core, respectively. With their results, we won second place in all the tasks of the challenge.},
    arxiv={2102.01363},
    pdf={https://dihardchallenge.github.io/dihard3/system_descriptions/dihard3_system_description_team115.pdf},
    slides={https://dihardchallenge.github.io/dihard3workshop/slide/Hitachi-JHU%20System%20for%20the%20Third%20DIHARD%20Speech%20Diarization%20Challenge.pdf}
}
@inproceedings{morishita2020hitachi,
    title={Hitachi at {SemEval-2020} Task 8: Simple but Effective Modality Ensemble for Meme Emotion Recognition},
    author={Morishita, Terufumi and Morio, Gaku and Horiguchi, Shota and Ozaki, Hiroaki and Miyoshi, Toshinori},
    booktitle={The Forteenth Workshop on Semantic Evaluation (SemEval)},
    year={2020},
    pages={1126–-1134},
    abbr={SemEval},
    abstract={Users of social networking services often share their emotions via multi-modal content, usually images paired with text embedded in them. SemEval-2020 task 8, Memotion Analysis, aims at automatically recognizing these emotions of so-called internet memes. In this paper, we propose a simple but effective Modality Ensemble that incorporates visual and textual deep-learning models, which are independently trained, rather than providing a single multi-modal joint network. To this end, we first fine-tune four pre-trained visual models (i.e., Inception-ResNet, PolyNet, SENet, and PNASNet) and four textual models (i.e., BERT, GPT-2, Transformer-XL, and XLNet). Then, we fuse their predictions with ensemble methods to effectively capture cross-modal correlations. The experiments performed on dev-set show that both visual and textual features aided each other, especially in subtask-C, and consequently, our system ranked 2nd on subtask-C.},
    html={https://aclanthology.org/2020.semeval-1.149}
}
@inproceedings{kanda2018hitachijhu,
    title={The {Hitachi/JHU} {CHiME-5} system: Advances in speech recognition for everyday home environments using multiple microphone arrays},
    author={Naoyuki Kanda and Rintaro Ikeshita and Shota Horiguchi and Yusuke Fujita and Kenji Nagamatsu and Xiaofei Wang and Vimal Manohar and Nelson Enrique {Yalta Soplin} and Matthew Maciejewski and Szu-Jui Chen and Aswin Shanmugam Subramanian and Ruizhi Li and Zhiqi Wang and Jason Naradowsky and L. Paola Garcia-Perera and Gregory Sell},
    year={2018},
    pages={6--10},
    abbr={CHiME},
    abstract={This paper presents Hitachi and JHU’s efforts on developing CHiME-5 system to recognize dinner party speeches recorded by multiple microphone arrays. We newly developed (1) the way to apply multiple data augmentation methods, (2) residual bidirectional long short-term memory, (3) 4-ch acoustic models, (4) multiple-array combination methods, (5) hypothesis deduplication method, and (6) speaker adaptation technique of neural beamformer. As the results, our best system in category B achieved 52.38% of word error rates (WERs) for development set, which corresponded to 35% of relative WER reduction from the state-of-the-art baseline. Our best system also achieved 48.20% of WER for evaluation set, which was the 2nd best result in the CHiME-5 competition.},
    html={https://www.isca-speech.org/archive/chime_2018/kanda18_chime.html}
}